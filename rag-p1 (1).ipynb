{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:10:10.502445Z","iopub.execute_input":"2025-11-18T03:10:10.502682Z","iopub.status.idle":"2025-11-18T03:10:10.509454Z","shell.execute_reply.started":"2025-11-18T03:10:10.502656Z","shell.execute_reply":"2025-11-18T03:10:10.508690Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install langchain langchain-community faiss-cpu sentence-transformers torch transformers datasets pandas tqdm -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:18:38.018841Z","iopub.execute_input":"2025-11-18T03:18:38.019331Z","iopub.status.idle":"2025-11-18T03:20:14.406175Z","shell.execute_reply.started":"2025-11-18T03:18:38.019305Z","shell.execute_reply":"2025-11-18T03:20:14.405368Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Load Hugging Face documentation dataset (perfect for AI/ML RAG)\n# This dataset contains real HuggingFace API documentation with explanations\ndataset = load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n\nprint(f\"Dataset size: {len(dataset)} documents\")\nprint(f\"Sample document:\\n{dataset[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:10:10.511076Z","iopub.execute_input":"2025-11-18T03:10:10.511333Z","iopub.status.idle":"2025-11-18T03:10:23.741798Z","shell.execute_reply.started":"2025-11-18T03:10:10.511311Z","shell.execute_reply":"2025-11-18T03:10:23.741197Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7d96db1686b459f879eae9ae4b961a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"882b9d673f8f426180d6459b69cd25e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00307bc634824b02bbd10e665c91c688"}},"metadata":{}},{"name":"stdout","text":"Dataset size: 2647 documents\nSample document:\n{'text': ' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget ğŸ ğŸ‰!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n', 'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx'}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Convert to DataFrame for easier processing\ndf = pd.DataFrame({\n    'text': [doc['text'] for doc in dataset],\n    'source': [doc['source'] for doc in dataset]\n})\nprint(f\"\\nDataFrame shape: {df.shape}\")\nprint(f\"First few sources:\\n{df['source'].unique()[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:10:30.635511Z","iopub.execute_input":"2025-11-18T03:10:30.635784Z","iopub.status.idle":"2025-11-18T03:10:30.817360Z","shell.execute_reply.started":"2025-11-18T03:10:30.635764Z","shell.execute_reply":"2025-11-18T03:10:30.816739Z"}},"outputs":[{"name":"stdout","text":"\nDataFrame shape: (2647, 2)\nFirst few sources:\n['huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx'\n 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx'\n 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md'\n 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md'\n 'gradio-app/gradio/blob/main/demo/blocks_random_slider/run.ipynb']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**Data Preprocessing and Chunking**","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document as LangchainDocument\n\n# Remove documents with missing text\ndf_clean = df.dropna(subset=['text'])\nprint(f\"Documents after cleaning: {len(df_clean)}\")\n\n# Create LangChain Document objects\ndocuments = [\n    LangchainDocument(page_content=row['text'], metadata={\"source\": row['source']})\n    for _, row in df_clean.iterrows()\n]\n\n# Split documents into chunks for better retrieval\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=512,           # Optimal size for embeddings\n    chunk_overlap=100,         # Overlap for context preservation\n    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n)\n\ndocs_processed = []\nfor doc in tqdm(documents[:500], desc=\"Chunking documents\"):  # Use first 500 for demo\n    chunks = text_splitter.split_documents([doc])\n    docs_processed.extend(chunks)\n\nprint(f\"Total chunks created: {len(docs_processed)}\")\nprint(f\"Sample chunk:\\n{docs_processed[0].page_content[:200]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:10:34.913580Z","iopub.execute_input":"2025-11-18T03:10:34.914289Z","iopub.status.idle":"2025-11-18T03:10:35.851869Z","shell.execute_reply.started":"2025-11-18T03:10:34.914264Z","shell.execute_reply":"2025-11-18T03:10:35.851235Z"}},"outputs":[{"name":"stdout","text":"Documents after cleaning: 2647\n","output_type":"stream"},{"name":"stderr","text":"Chunking documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 3426.04it/s]","output_type":"stream"},{"name":"stdout","text":"Total chunks created: 12474\nSample chunk:\nCreate an Endpoint\n\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**Creating Embeddings using Sentence Transformers**","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Use a lightweight but powerful embedding model\n# This model is optimized for semantic similarity\nEMBEDDING_MODEL = \"all-mpnet-base-v2\"\n\nprint(f\"Loading embedding model: {EMBEDDING_MODEL}\")\nembedding_model = SentenceTransformer(EMBEDDING_MODEL)\n\n# Embed all chunks\nprint(\"Creating embeddings for all chunks...\")\nchunk_texts = [doc.page_content for doc in docs_processed]\nembeddings = embedding_model.encode(\n    chunk_texts,\n    batch_size=4,\n    show_progress_bar=True,\n    convert_to_numpy=True\n)\n\nprint(f\"Embeddings shape: {embeddings.shape}\")\nprint(f\"Embedding dimension: {embeddings.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:10:57.584605Z","iopub.execute_input":"2025-11-18T03:10:57.584880Z","iopub.status.idle":"2025-11-18T03:14:10.197904Z","shell.execute_reply.started":"2025-11-18T03:10:57.584860Z","shell.execute_reply":"2025-11-18T03:14:10.197081Z"}},"outputs":[{"name":"stderr","text":"2025-11-18 03:11:11.325052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763435471.785241      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763435471.890508      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loading embedding model: all-mpnet-base-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66737f72ba074d0e8521e2efb4c3edbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f320bf4bb0541ec846ea78fc69ccd7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf4c70174d04402b9d59b756a0f1a761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0723da23469b4a0a84a9f6debb6dd970"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e0e965c05a4d68a96c688148309179"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b85155069f7b46c4a42809d5dbe96127"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b41e3441285434282ded86450017abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"582391166acf4d87a0849dd856d60868"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf23604853af417ab3483dfa7249fee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42916d4b991f4632a3e14e94322757cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b0aae11af564f31b9980025f49520db"}},"metadata":{}},{"name":"stdout","text":"Creating embeddings for all chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3119 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a31ea343ea14480da1da600c95d1aac3"}},"metadata":{}},{"name":"stdout","text":"Embeddings shape: (12474, 768)\nEmbedding dimension: 768\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**Building FAISS Vector Database**","metadata":{}},{"cell_type":"code","source":"import faiss\nfrom langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\n# Create HuggingFace embeddings wrapper for LangChain\nhf_embeddings = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL,\n    model_kwargs={\"device\": \"cuda\"},  # Use \"cuda\" if GPU available\n    encode_kwargs={\"normalize_embeddings\": True}  # For cosine similarity\n)\n\n# Build FAISS vector database\nprint(\"Building FAISS vector database...\")\nvector_db = FAISS.from_documents(\n    docs_processed,\n    hf_embeddings,\n    distance_strategy=DistanceStrategy.COSINE\n)\n\nprint(\"âœ… FAISS database created successfully!\")\n\n# Save for later use\nvector_db.save_local(\"faiss_index\")\nprint(\"âœ… Database saved locally!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:20:18.531622Z","iopub.execute_input":"2025-11-18T03:20:18.532492Z","iopub.status.idle":"2025-11-18T03:23:19.643869Z","shell.execute_reply.started":"2025-11-18T03:20:18.532455Z","shell.execute_reply":"2025-11-18T03:23:19.642945Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_48/3118961586.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  hf_embeddings = HuggingFaceEmbeddings(\n","output_type":"stream"},{"name":"stdout","text":"Building FAISS vector database...\nâœ… FAISS database created successfully!\nâœ… Database saved locally!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"**Loading LLM for answer generation**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nimport torch\n# Use an efficient open-source LLM\nLLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n\nprint(f\"Loading LLM: {LLM_MODEL}\")\n\n# Check device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Create text generation pipeline\nllm_pipeline = pipeline(\n    \"text-generation\",\n    model=LLM_MODEL,\n    device=1 if device == \"cuda\" else -1,\n    torch_dtype=torch.float16,\n    max_new_tokens=128,\n    temperature=0.3,# this is for more deterministic answers\n    num_beams=1, # Beam search is for better output\n    top_p=0.95\n)\n\nprint(\"âœ… LLM loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:25:01.965430Z","iopub.execute_input":"2025-11-18T03:25:01.966300Z","iopub.status.idle":"2025-11-18T03:26:41.820464Z","shell.execute_reply.started":"2025-11-18T03:25:01.966271Z","shell.execute_reply":"2025-11-18T03:26:41.819413Z"}},"outputs":[{"name":"stdout","text":"Loading LLM: mistralai/Mistral-7B-Instruct-v0.1\nUsing device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42578d1b81934db58965aaa481a41a01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb22c782ade14b358236bf04ffc3facb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f2da5bce9e0444fac35539ca48c22f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d12578f9d14fcc8676d78ffcf271a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e3402dee3cd465da4bc7a67b66a08bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8acfcb12290745c9954f2aa9e0cb7b33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e01551e797964e4f8d2675ad78f4e6c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11699f7ef3048599281dac9220a31da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58315499b11a467db029c1729c78cf93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecf21fea36f24bba850e20752527b05f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d318ebfebecc4d7b86ca46c709260eb9"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:1\n","output_type":"stream"},{"name":"stdout","text":"âœ… LLM loaded successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Define RAG prompt template\nRAG_PROMPT_TEMPLATE = \"\"\"\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer concisely based only on the above context. If the answer is not found, respond: \"I don't know.\"\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:48:34.033715Z","iopub.execute_input":"2025-11-18T03:48:34.034534Z","iopub.status.idle":"2025-11-18T03:48:34.038215Z","shell.execute_reply.started":"2025-11-18T03:48:34.034507Z","shell.execute_reply":"2025-11-18T03:48:34.037361Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**RAG Pipeline**","metadata":{}},{"cell_type":"code","source":"from typing import Tuple, List\n\ndef rag_retrieve_and_generate(\n    question: str,\n    vector_db,\n    llm_pipeline,\n    k: int = 5\n) -> Tuple[str, List]:\n    \"\"\"\n    Complete RAG pipeline: retrieve relevant docs and generate answer\n    \"\"\"\n    # Step 1: Retrieve relevant documents\n    print(f\"ğŸ” Retrieving documents for: {question}\")\n    retrieved_docs = vector_db.similarity_search(question, k=k)\n    \n    # Step 2: Prepare context\n    context = \"\\n\".join([\n        f\"Source {i+1}: {doc.page_content[:300]}...\\n(Source: {doc.metadata.get('source', 'Unknown')})\\n\"\n        for i, doc in enumerate(retrieved_docs)\n    ])\n    \n    # Step 3: Build prompt\n    prompt = RAG_PROMPT_TEMPLATE.format(\n        context=context,\n        question=question\n    )\n    \n    # Step 4: Generate answer\n    print(\"ğŸ’­ Generating answer...\")\n    response = llm_pipeline(prompt, max_new_tokens=256)\n    answer = response[0]['generated_text'].split(\"Answer:\")[-1].strip()\n    \n    return answer, retrieved_docs\n\n# Test the RAG system\ntest_question = \"What are transformers in machine learning?\"\nanswer, sources = rag_retrieve_and_generate(\n    test_question,\n    vector_db,\n    llm_pipeline,\n    k=5\n)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Question: {test_question}\")\nprint(f\"{'='*60}\")\nprint(f\"Answer:\\n{answer}\")\nprint(f\"{'='*60}\")\nprint(f\"Retrieved Sources:\")\nfor i, doc in enumerate(sources):\n    print(f\"{i+1}. {doc.metadata.get('source', 'Unknown')}\")\n    print(f\"   Content preview: {doc.page_content[:150]}...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:48:43.021036Z","iopub.execute_input":"2025-11-18T03:48:43.021677Z","iopub.status.idle":"2025-11-18T03:48:48.156497Z","shell.execute_reply.started":"2025-11-18T03:48:43.021641Z","shell.execute_reply":"2025-11-18T03:48:48.155853Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ” Retrieving documents for: What are transformers in machine learning?\nğŸ’­ Generating answer...\n\n============================================================\nQuestion: What are transformers in machine learning?\n============================================================\nAnswer:\nTransformers are a library for machine learning that allows for tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering. They also include an opinionated design and operations such as tensor contractions.\n============================================================\nRetrieved Sources:\n1. huggingface/blog/blob/main/lewis-tunstall-interview.md\n   Content preview: And if you actually look at what's needed to make this conversion happen in the transformers library, it's fairly gnarly. But we make it so that you o...\n2. huggingface/transformers/blob/main/README.md\n   Content preview: Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, informa...\n3. huggingface/blog/blob/main/lewis-tunstall-interview.md\n   Content preview: ### That's very cool. Have there been, any standout applications of transformers?\n\n**Lewis:** I think there are a few. One is maybe emotional or perso...\n4. huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md\n   Content preview: General overview of ğŸ¤— Transformers\n----------------------------------\n\nFirst, you should get a general overview of ğŸ¤— Transformers. Transformers \nis a ...\n5. huggingface/transformers/blob/main/docs/source/en/model_memory_anatomy.md\n   Content preview: ## Anatomy of Model's Operations\n\nTransformers architecture includes 3 main groups of operations grouped below by compute-intensity.\n\n1. **Tensor Cont...\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"**Reranking for Quality**","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\n\n# Add reranking to improve retrieval quality\nreranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n\ndef rag_with_reranking(\n    question: str,\n    vector_db,\n    llm_pipeline,\n    reranker,\n    k_retrieve: int = 15,\n    k_final: int = 5\n) -> Tuple[str, List]:\n    \"\"\"\n    Advanced RAG with reranking: retrieve more docs, rerank, then generate\n    \"\"\"\n    # Retrieve more documents initially\n    retrieved_docs = vector_db.similarity_search(question, k=k_retrieve)\n    \n    # Rerank using cross-encoder\n    doc_text_pairs = [(question, doc.page_content) for doc in retrieved_docs]\n    scores = reranker.predict(doc_text_pairs)\n    \n    # Keep top-k after reranking\n    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k_final]\n    reranked_docs = [retrieved_docs[i] for i in ranked_indices]\n    \n    # Prepare context and generate answer\n    context = \"\\n\".join([\n        f\"Document {i+1}: {doc.page_content[:400]}...\\n\"\n        for i, doc in enumerate(reranked_docs)\n    ])\n    \n    prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=question)\n    response = llm_pipeline(prompt)\n    answer = response[0]['generated_text'].split(\"Answer:\")[-1].strip()\n    \n    return answer, reranked_docs\n\n# Test reranking\nanswer_reranked, sources_reranked = rag_with_reranking(\n    \"What is transfer learning in neural networks?\",\n    vector_db,\n    llm_pipeline,\n    reranker\n)\n\nprint(f\"Answer (with reranking):\\n{answer_reranked}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:48:58.330773Z","iopub.execute_input":"2025-11-18T03:48:58.331128Z","iopub.status.idle":"2025-11-18T03:49:03.188709Z","shell.execute_reply.started":"2025-11-18T03:48:58.331104Z","shell.execute_reply":"2025-11-18T03:49:03.187883Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Answer (with reranking):\nTransfer learning in neural networks is the idea of leveraging the knowledge acquired by a model trained with lots of data on another task. This allows for better results on a new task with less computation, time, and data required.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"**Evaluation Metrics**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef evaluate_rag_quality(question: str, answer: str, retrieved_docs, embeddings_model):\n    \"\"\"\n    Evaluate RAG system quality with metrics\n    \"\"\"\n    # Question-Answer relevance\n    qa_embedding = embeddings_model.encode(f\"{question} {answer}\")\n    doc_embeddings = embeddings_model.encode([doc.page_content for doc in retrieved_docs])\n    \n    qa_relevance = cosine_similarity([qa_embedding], doc_embeddings).mean()\n    \n    # Question-Document relevance\n    question_embedding = embeddings_model.encode(question)\n    doc_relevance = cosine_similarity([question_embedding], doc_embeddings).mean()\n    \n    return {\n        \"qa_relevance\": round(qa_relevance, 4),\n        \"doc_relevance\": round(doc_relevance, 4),\n        \"retrieval_count\": len(retrieved_docs)\n    }\n\n# Test evaluation\nmetrics = evaluate_rag_quality(\n    \"What are transformers?\",\n    answer,\n    sources,\n    embedding_model\n)\n\nprint(f\"\\nğŸ“Š RAG Quality Metrics:\")\nprint(f\"  QA Relevance Score: {metrics['qa_relevance']} (higher is better)\")\nprint(f\"  Document Relevance: {metrics['doc_relevance']} (higher is better)\")\nprint(f\"  Documents Retrieved: {metrics['retrieval_count']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:28:37.271588Z","iopub.execute_input":"2025-11-18T03:28:37.272332Z","iopub.status.idle":"2025-11-18T03:28:37.413428Z","shell.execute_reply.started":"2025-11-18T03:28:37.272303Z","shell.execute_reply":"2025-11-18T03:28:37.412773Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“Š RAG Quality Metrics:\n  QA Relevance Score: 0.7024999856948853 (higher is better)\n  Document Relevance: 0.4977000057697296 (higher is better)\n  Documents Retrieved: 5\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def interactive_rag_demo():\n    \"\"\"\n    Interactive demo for testing RAG system\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ¤– AI/ML Documentation RAG System\")\n    print(\"=\"*60)\n    \n    test_questions = [\n        \"How do I use transformers for NLP tasks?\",\n        \"What is transfer learning?\",\n        \"Explain attention mechanisms in detail\",\n        \"How do I fine-tune a pretrained model?\",\n        \"What are embeddings and why are they useful?\"\n    ]\n    \n    for question in test_questions:\n        print(f\"\\nğŸ“Œ Question: {question}\")\n        print(\"-\" * 60)\n        \n        answer, retrieved_docs = rag_retrieve_and_generate(\n            question,\n            vector_db,\n            llm_pipeline,\n            k=3\n        )\n        \n        metrics = evaluate_rag_quality(question, answer, retrieved_docs, embedding_model)\n        \n        print(f\"âœ… Answer: {answer[:300]}...\")\n        print(f\"ğŸ“Š Relevance Score: {metrics['doc_relevance']}\")\n        print()\n\n# Run demo\ninteractive_rag_demo()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:49:55.680146Z","iopub.execute_input":"2025-11-18T03:49:55.680439Z","iopub.status.idle":"2025-11-18T03:50:26.783493Z","shell.execute_reply.started":"2025-11-18T03:49:55.680419Z","shell.execute_reply":"2025-11-18T03:50:26.782633Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nğŸ¤– AI/ML Documentation RAG System\n============================================================\n\nğŸ“Œ Question: How do I use transformers for NLP tasks?\n------------------------------------------------------------\nğŸ” Retrieving documents for: How do I use transformers for NLP tasks?\nğŸ’­ Generating answer...\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"âœ… Answer: To use transformers for NLP tasks, you can use the Hugging Face Transformers library. This library provides pre-trained models for a wide range of NLP tasks, as well as tools for training your own models from scratch. You can also use the library to efficiently test large NLP models and integrate Py...\nğŸ“Š Relevance Score: 0.794700026512146\n\n\nğŸ“Œ Question: What is transfer learning?\n------------------------------------------------------------\nğŸ” Retrieving documents for: What is transfer learning?\nğŸ’­ Generating answer...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"âœ… Answer: Transfer learning is leveraging the knowledge acquired by a model trained with lots of data on another task....\nğŸ“Š Relevance Score: 0.6869000196456909\n\n\nğŸ“Œ Question: Explain attention mechanisms in detail\n------------------------------------------------------------\nğŸ” Retrieving documents for: Explain attention mechanisms in detail\nğŸ’­ Generating answer...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"âœ… Answer: Attention mechanisms in transformer models allow the model to focus on specific parts of the input sequence while processing it. The attention scores are formulated based on the similarity between the query and key vectors, and the resulting attention weights are used to weight the values vector. Th...\nğŸ“Š Relevance Score: 0.6306999921798706\n\n\nğŸ“Œ Question: How do I fine-tune a pretrained model?\n------------------------------------------------------------\nğŸ” Retrieving documents for: How do I fine-tune a pretrained model?\nğŸ’­ Generating answer...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"âœ… Answer: To fine-tune a pretrained model, you need to upload \"Training Data\" in CSV file format. The dataset should be correctly formatted. An example of the required format can be found [here](https://huggingface.co/docs/autotrain/main/en/llm_finetuning). You can also fine-tune a purely pretrained model ins...\nğŸ“Š Relevance Score: 0.7017999887466431\n\n\nğŸ“Œ Question: What are embeddings and why are they useful?\n------------------------------------------------------------\nğŸ” Retrieving documents for: What are embeddings and why are they useful?\nğŸ’­ Generating answer...\nâœ… Answer: Embeddings are key for several industrial applications such as Google searches, Snapchat, Facebook, and more. They allow you to build things like chatbots, recommendation systems, zero-shot classifiers, image search, FAQ systems, and more. They are useful because they allow you to match text to text...\nğŸ“Š Relevance Score: 0.6330000162124634\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import json\n\n# Save RAG metadata for reproducibility\nrag_metadata = {\n    \"embedding_model\": EMBEDDING_MODEL,\n    \"llm_model\": LLM_MODEL,\n    \"dataset\": \"m-ric/huggingface_doc\",\n    \"chunks_created\": len(docs_processed),\n    \"vector_db_type\": \"FAISS\",\n    \"distance_metric\": \"cosine\"\n}\n\nwith open(\"rag_metadata.json\", \"w\") as f:\n    json.dump(rag_metadata, f, indent=2)\n\nprint(\"âœ… RAG system ready for deployment!\")\nprint(f\"ğŸ“¦ Saved files: faiss_index/, rag_metadata.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:53:00.090804Z","iopub.execute_input":"2025-11-18T03:53:00.091362Z","iopub.status.idle":"2025-11-18T03:53:00.096991Z","shell.execute_reply.started":"2025-11-18T03:53:00.091336Z","shell.execute_reply":"2025-11-18T03:53:00.096384Z"}},"outputs":[{"name":"stdout","text":"âœ… RAG system ready for deployment!\nğŸ“¦ Saved files: faiss_index/, rag_metadata.json\n","output_type":"stream"}],"execution_count":20}]}